---
title: 强化学习（强化学习方法分类）
date: 2022-11-26 11:24:19 +0800
categories: [Academic, Knowledge]
tags: [python]
math: true
---

本文介绍了强化学习的方法分类，主要包括基于模型的方法换个无模型的方法。其中，基于模型的方法分为策略迭代、值迭代、策略搜索，无模型的方法包括蒙特卡洛方法和时间差分法。

<!--more-->

---

## 1. 强化学习的基本问题

- **预测问题**，也即预测给定策略的状态价值函数。给定强化学习的6个要素：状态集$S$, 动作集$A$, 模型状态转移概率矩阵$P$, 即时奖励$R$，衰减因子$\gamma$,  给定策略$\pi$， 求解该策略的状态价值函数$v_\pi(s)$；
- **控制问题**，也即求解最优的价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 模型状态转移概率矩阵$P$, 即时奖励$R$，衰减因子$\gamma$, 求解最优的状态价值函数$v_*$和最优策略$\pi_*$。　

## 2. 基于模型的方法

基于模型的方法要求马尔可夫决策过程五元组完全已知，即 $<S,A,P,R,\gamma>$ 是完全确定的。

### 2.1. 动态规划方法

动态规划（Dynamic Programming，DP）是一种将复杂问题简单化的思想，而不是指某种具体的算法。DP算法通过把复杂问题分解为子问题，通过求解子问题进而得到整个问题的解。在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。

一个复杂问题可以使用DP思想来求解，只要满足两个性质就可以：(1)一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解；(2)子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。

巧了，强化学习要解决的问题刚好满足这两个条件。还记得贝尔曼方程吗？

$$
\begin{aligned}
v(s)& \leftarrow\mathbb{E}_\pi[R_{t+1}+\gamma v(s^\prime)]\\
\Rightarrow v(s) & = \sum_{a\in A}\pi(a\vert s)[R_s^a+\gamma \sum_{s^\prime \in S}P_{ss^\prime}^a v(s^\prime)]\\
\end{aligned}
$$

不难发现，当模型已知时（即 $A,P_{ss^\prime}^a, R_s^a$ 已知），我们可以定义出子问题求解每个状态的状态价值函数，同时这个式子又是一个递推的式子, 意味着利用它，我们可以使用上一个迭代周期内的状态价值来计算更新当前迭代周期某状态 $s$ 的状态价值（详见策略迭代）。可见，使用动态规划来求解强化学习问题是比较自然的。

> 此处有一个概念：值函数的计算用到了bootstapping的方法。所谓bootstrpping本意是指自举，此处是指当前值函数的计算用到了后继状态的值函数，也即用后继状态的值函数计算当前值函数。

#### 2.1.1. 策略迭代

知道了动态规划与强化学习的联系，我们就能用DP的思想去求解强化学习问题。策略迭代包括策略评估(Policy Evaluation)和策略改进(Policy Improvement)，其基本过程是从一个初始化的策略出发，先进行策略评估，然后策略改进，评估改进的策略，再进一步改进策略，经过不断迭代更新，直到策略收敛。下面具体介绍策略评估和策略改进。

- **策略评估**

策略评估的基本思路是从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼方程、状态转移概率和奖励，同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。

假设给定一个策略 $\pi$，第 $k$ 轮迭代，已经计算出所有状态价值，则第 $k+1$ 轮迭代可以结合贝尔曼方程计算状态价值如下

$$
v_{\color{red}{k+1}}(s)  = \sum_{a\in A}\pi(a\vert s)[R_s^a+\gamma \sum_{s^\prime \in S}P_{ss^\prime}^a v_{\color{red}k}(s^\prime)]
$$

下面给出一个经典的 Grid World 例子。假设有一个4x4的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该两个格子，则停止移动，此后每轮奖励都是0。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。

![Grid World](/assets/img/postsimg/20221126/gridworld.png)

下面对问题进行定义：
- $R_s^a=-1$，即个体在16宫格其他格的每次移动，得到的**即时奖励**都为-1。
- $\gamma=1$，即**奖励的累计不衰减**。
- $P_{ss^\prime}^a=1$，即每次移动到的下一格都是固定的（如往上走一定到上面的格子），**不考虑转移不确定**的情况；
- $\pi(a\vert s) = 0.25, \forall a\in A$，即**给定随机策略**，每个格子里有25%的概率向周围的4个格子移动。

至此，马尔可夫决策过程的所有参数均已知，下面进行状态价值函数进行预测。

- **策略改进**

#### 2.1.2. 值迭代

#### 2.1.3. 策略搜索

## 3. 参考文献

[1] 量子位. [知乎：能否介绍一下强化学习（Reinforcement Learning），以及与监督学习的不同？](https://www.zhihu.com/question/41775291).

[2] ReEchooo. [强化学习知识要点与编程实践（1）——马尔可夫决策过程](https://blog.csdn.net/qq_41773233/article/details/114698902)

[3] ReEchooo. [强化学习笔记（2）——马尔可夫决策过程](https://blog.csdn.net/qq_41773233/article/details/114435113)

[4] Ping2021. [第二讲 马尔可夫决策过程](https://zhuanlan.zhihu.com/p/494755866)

[5] 木头人puppet. [强化学习：贝尔曼方程和最优性](https://www.jianshu.com/p/9878238a1c9e)

[6] koch. [强化学习-贝尔曼方程和贝尔曼最优方程的推导](https://zhuanlan.zhihu.com/p/505723322)