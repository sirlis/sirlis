---
title: 强化学习
date: 2022-11-09 112:36:19 +0800
categories: [Academic, Knowledge]
tags: [python]
math: true
---

本文介绍了强化学习的基本概念和模型。

<!--more-->

---

- [1. 强化学习](#1-强化学习)
- [2. 马尔可夫过程](#2-马尔可夫过程)
- [3. 马尔可夫奖励过程](#3-马尔可夫奖励过程)
- [4. 马尔可夫决策过程](#4-马尔可夫决策过程)
- [5. 参考文献](#5-参考文献)

## 1. 强化学习


强化学习是机器学习领域之一，受到行为心理学的启发，主要关注智能体如何在环境中采取不同的行动，以最大限度地提高累积奖励。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。

强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。

假设体当前时刻 $t$ 环境状态记为 $s_t$，收到奖励 $r_t$，此时智能体采取动作 $a_t$，使得下一时刻 $t+1$ 环境达到了新状态 $s_{t+1}$，在新的状态下环境产生了反馈奖励 $r_{t+1}$ 给智能体。智能体根据新状态 $s_{t+1}$ 和反馈奖励 $r_{t+1}$ ，执行新动作 $a_{t+1}$，如此反复迭代交互。

![强化学习示意图](/assets/img/postsimg/20221109/0-reinforcement-learning-basic-diagram.jpg)


重复进行这个交互过程，智能体就可以得到一系列状态，动作和奖励，称为当前策略的一个轨迹（trajectory）。这个轨迹可以拆分成一个一个的 ，将其称为状态转移样本。如果这个交互过程是周期的，那么智能体与环境交互一定的时间步之后，整个交互过程会重置。如果这个交互过程是无限期的，那么智能体与环境可以一直交互下去，直到触发终止条件。

上述过程的最终目的，是让智能体最大化累计奖励（Cumulative Award），累计奖励为 $G$，有

$$
G = r_1+r_2+...+r_n
$$
## 2. 马尔可夫过程

马尔可夫过程（Markov Process）：

- 在一个时序过程中，如果 $t＋1$ 时刻的状态仅取决于 $t$ 时刻的状态 $s_t$ 而与 $t$ 时刻之前的任何状态都无关时，则认为 $t$ 时刻的状态 $s_t$ 具有马尔可夫性(Markov property)；

- 若过程中的每一个状态都具有马尔科夫性，则这个过程具备马尔科夫性。具备了马尔科夫性的随机过程称为马尔科夫过程，又称马尔科夫链（Markov chain）。

马尔可夫过程的关键在于状态转移概率

$$
P_{s s^\prime}=\mathbb{P}[S_{t+1}=s^\prime \vert s_{t}=s]
$$

从上式也可以看到，下一时刻的状态$S_{t+1}$ 仅与当前时刻的状态 $S_t$ 有关，而与 $S_1,\cdots,S_{t-1}$ 无关。注意：这里的记号非常严谨， $S_{t}, S_{t+1}$ 代表某一时刻的状态，而 $s,s^\prime$ 代表某一种具体的状态类型。

为了描述整个状态空间中不同类型状态之间的关系，用矩阵表示，即为：

$$
P=
\begin{bmatrix}
P(s_1\vert s_1) & P(s_2\vert s_1) &\dots & P(s_n\vert s_1)\\
P(s_1\vert s_2) & P(s_2\vert s_2) &\dots & P(s_n\vert s_2)\\
\vdots & \vdots & \ddots &\vdots\\
P(s_1\vert s_n) & P(s_2\vert s_n)&\dots & P(s_n\vert s_n)\\
\end{bmatrix}
$$

其中，$P(s_j\vert s_i)$ 表示状态从 $s_i$ 转移到 $s_j$ 的状态转移概率，显然状态转移概率矩阵 $P$ 的规模是所有状态类型数 $n$ 的平方。

表述一个马尔可夫过程，只需要元组 $<S,P>$ 即可。

![马尔可夫过程](/assets/img/postsimg/20221109/1-mp.png)

不难写出状态转移概率矩阵：

$$
\begin{aligned}
&\quad C1\;\;\;C2\;\;\;C3\;\;\;Pass\;Pub\;FB\;\;Sleep\\
P=
\begin{array}{r}
    C1\\
    C2\\
    C3\\
    Pass\\
    Pub \\
    FB\\
    Sleep
\end{array}
&\begin{bmatrix}
    0& 0.5 &0&0&0&0.5&0\\
    0&0&0.8&0&0&0&0.2\\
    0&0&0&0.6&0.4&0&0\\
    0&0&0&0&0&0&1\\
    0.2&0.4&0.4&0&0&0&0\\
    0.1&0&0&0&0&0.9&0\\
    0&0&0&0&0&0&1\\
\end{bmatrix}
\end{aligned}
$$

马尔可夫过程中的三个概念：

- **状态序列**（episode）：一个状态转换过程，包含很多状态。如：$s_1$-$s_2$-$s_3$；
- **完整的状态序列**（complete episode）：状态序列的最后一个状态是终止状态；
- **采样**（sample）：从符合马尔科夫过程给定的状态转移概率矩阵生成一个状态序列的过程。

## 3. 马尔可夫奖励过程

马尔可夫奖励过程就是在马尔可夫过程中增加了奖励 $R$ 项和奖励因子 $\gamma$。

表述一个马尔可夫奖励过程（Markov Reward Process），需要元组 $<S,P,R,\gamma>$，其中：

- $S$ 是一个有限状态集；
- $P$ 是集合中状态转移概率矩阵：$P_{s s^\prime}=\mathbb{P}[S_{t+1}=s^\prime \vert S_t = s]$；
- $R$ 是奖励函数：$R_s = \mathbb{E}[R_{t+1}\vert S_t=s]$
- $\gamma$ 是折扣因子：$\gamma \in [0,1]$。

如下图所示

![马尔可夫奖励过程](/assets/img/postsimg/20221109/2-mrp.png)

注意，在马尔可夫奖励过程中，奖励指的是离开一个状态或者进入一个状态的奖励（用来评价状态的好坏）。比如到达 $s_1=C_1$​ 状态时，可以获得 $-2$ 的奖励，到达 $s_4=Pass$​ 时，可以得到 $+10$ 的奖励，其它状态没有任何奖励。

**收益**（Return）：是一个马尔可夫奖励过程中，从某个状态 $s_t$ 开始采样直到终止状态时（一条完整的状态序列）所有奖励的折扣和：

$$
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$

越往后得到的奖励，折扣得越多。这说明我们其实更希望得到现有的奖励，未来的奖励就要把它打折扣。折扣因子可以作为强化学习的一个超参数来进行调整，折扣因子不同就会得到不同行为的智能体。

**价值**（Value）：对于马尔可夫奖励过程，状态价值函数被定义为 Return 的期望。也即从某个状态 $s_t$ 开始采样无数条完整状态序列后，其收益的平均值。

$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t=s]\\
&=\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...\vert S_t=s]\\
&=\mathbb{E}[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+...)\vert S_t=s]\\
&=\mathbb{E}[R_{t+1}+\gamma G_{t+1}\vert S_t=s]
\end{aligned}
$$

## 4. 马尔可夫决策过程

马尔可夫过程定义为一个六元组：

1. 所有状态的集合 $S$，也称为状态空间。状态空间的大小可以是有限的，也可以是无限的;
2. 动作的集合 $A$，也称为动作空间。动作空间同样可以是有限的，也可以是无限的;
3. 在状态之间转换的规则（转移概率矩阵） $P$；
4. 规定转换后“即时奖励”的规则（奖励函数）$R$；
5. 描述主体能够观察到什么的规则。

马尔可夫性质：

$$
P(s_t \vert s_0, a_0, s_2, a_1,...,s_{t-1},a_{t-1}) = P(s_t \vert s_{t-1},a_{t-1})
$$

表示当前状态 $s_t$ 只受到上一时刻状态 $s_{t-1}$ 和动作 $a_{t-1}$ 的影响。

## 5. 参考文献

[1] 知乎. [强化学习（Reinforcement Learning）](https://www.zhihu.com/topic/20039099/intro).
[2] ReEchooo. [强化学习知识要点与编程实践（1）——马尔可夫决策过程](https://blog.csdn.net/qq_41773233/article/details/114698902)