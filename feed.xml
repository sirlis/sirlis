

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>SIRLIS</title>
  <subtitle>sirlis</subtitle>
  <updated>2023-09-01T22:48:22+08:00</updated>
  <author>
    <name>sirlis</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="zh-CN"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator>
  <rights> © 2023 sirlis </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>航天中的四元数以及姿态运动学</title>
    <link href="http://localhost:4000/posts/space-quaternion/" rel="alternate" type="text/html" title="航天中的四元数以及姿态运动学" />
    <published>2023-06-02T18:38:19+08:00</published>
  
    <updated>2023-09-01T22:48:09+08:00</updated>
  
    <id>http://localhost:4000/posts/space-quaternion/</id>
    <content src="http://localhost:4000/posts/space-quaternion/" />
    <author>
      <name>sirlis</name>
    </author>

  
    
    <category term="Knowledge" />
    
  

  
    <summary>
      





      本文介绍了航天器姿态描述、姿态变换和姿态运动学中涉及的四元数表示法。






  1. 基础
    
      1.1. 矢量的正交分解
      1.2. 叉乘矩阵
      1.3. 坐标系定义
    
  
  2. 轴角旋转
  3. 姿态四元数
    
      3.1. 四元数定义
      3.2. 四元数表示旋转
      3.3. 姿态四元数
    
  
  4. 向量的坐标变换
  5. 姿态变换与四元数乘法
  6. 参考文献


1. 基础

1.1. 矢量的正交分解

对一个矢量​ $\boldsymbol{v}$ 进行沿单位参考轴 $\boldsymbol{e}$ ​正交分解为两个分量，分别为平行于 $\boldsymbol{e}$ ​的轴向分量和垂直于 $\boldsymbol{e}$ ​的垂直分量。如下图所示。



有

\...
    </summary>
  

  </entry>

  
  <entry>
    <title>Windows环境下使用MinGW-W64、CMake和NSIS打包C++工程</title>
    <link href="http://localhost:4000/posts/c-mingw64-compile-build-pack-with-cmake-nsis/" rel="alternate" type="text/html" title="Windows环境下使用MinGW-W64、CMake和NSIS打包C++工程" />
    <published>2023-05-15T23:59:19+08:00</published>
  
    <updated>2023-05-15T23:59:19+08:00</updated>
  
    <id>http://localhost:4000/posts/c-mingw64-compile-build-pack-with-cmake-nsis/</id>
    <content src="http://localhost:4000/posts/c-mingw64-compile-build-pack-with-cmake-nsis/" />
    <author>
      <name>sirlis</name>
    </author>

  
    
    <category term="Tutorial" />
    
    <category term="Coding" />
    
  

  
    <summary>
      





      本文介绍了 Windows 环境下使用 CMake（CPack） 和 NSIS 构建并打包 C/C++ 工程项目的基本流程和方法，核心在于 CMakeLists.txt 文件的编写。






  1. 引言
  2. CMake 配置
    
      2.1. 环境搭建
      2.2. 配置（Configure）
      2.3. 生成（Generate）
      2.4. 构建（Build）
      2.5. 运行和调试（Debug）
    
  
  3. 基于 CMake 的打包
    
      3.1. CPack
      3.2. NSIS
      3.3. 基于 CPack 和 NSIS 的打包
    
  
  4. 参考文献


1. 引言

前面我们介绍了在VSCode中采用MinGW-W64作为C/C++的编译器来进行...
    </summary>
  

  </entry>

  
  <entry>
    <title>强化学习（时序差分法）</title>
    <link href="http://localhost:4000/posts/reinforcement-learning-Temporal-Differences/" rel="alternate" type="text/html" title="强化学习（时序差分法）" />
    <published>2022-12-18T14:59:19+08:00</published>
  
    <updated>2022-12-18T14:59:19+08:00</updated>
  
    <id>http://localhost:4000/posts/reinforcement-learning-Temporal-Differences/</id>
    <content src="http://localhost:4000/posts/reinforcement-learning-Temporal-Differences/" />
    <author>
      <name>sirlis</name>
    </author>

  
    
    <category term="Academic" />
    
    <category term="Knowledge" />
    
  

  
    <summary>
      





      本文介绍了强化学习的时序差分法（Temporal-Difference, TD）。






  1. 引言
    
      1.1. 同轨策略下的时序差分控制（SARSA）
      1.2. 离轨策略下的时序差分控制（Q-Learning）
      1.3. 期望SARSA
    
  
  2. 参考文献


1. 引言

回顾强化学习的目标：价值估计（预测问题）和策略寻优（控制问题）。在前面的的介绍中，我们分别介绍了两种基于价值的方法，动态规划法和蒙特卡洛法：


  动态规划法（DP）：是 model-based 方法，包含策略评估和策略改进两步，策略评估用来进行价值估计（即预测问题），策略改进用来进行策略寻优（控制问题）。
  蒙特卡洛法（MC）：是 model-free 方法，因为一般情况下我们无法得到具体模型（状态转移概率），因此通过采样完整序列后，通...
    </summary>
  

  </entry>

  
  <entry>
    <title>强化学习（蒙特卡洛法）</title>
    <link href="http://localhost:4000/posts/reinforcement-learning-Monte-Carlo/" rel="alternate" type="text/html" title="强化学习（蒙特卡洛法）" />
    <published>2022-11-29T17:07:19+08:00</published>
  
    <updated>2022-11-29T17:07:19+08:00</updated>
  
    <id>http://localhost:4000/posts/reinforcement-learning-Monte-Carlo/</id>
    <content src="http://localhost:4000/posts/reinforcement-learning-Monte-Carlo/" />
    <author>
      <name>sirlis</name>
    </author>

  
    
    <category term="Academic" />
    
    <category term="Knowledge" />
    
  

  
    <summary>
      





      本文介绍了强化学习的 model-free 方法——蒙特卡洛法。






  1. 引言
  2. 蒙特卡洛法
    
      2.1. 大数定律
      2.2. 蒙特卡洛法
      2.3. 蒙特卡洛价值估计
      2.4. 增量更新方法
    
  
  3. 参考文献


1. 引言

在前面的 model-based 动态规划方法中，我们假设已知模型的动态特性 $p(s^\prime,r \vert s,a)$，此时可以对下一步的状态和回报做出预测。而在很多实际案例中，我们无法得知模型的动态特性，此时动态规划方法就不适用了。

2. 蒙特卡洛法

2.1. 大数定律

大数定律的定义是，当随机事件发生的次数足够多时，随机事件发生的频率趋近于预期的概率。可以简单理解为样本数量越多，其平概率越接近于期望值。大数定律的条件：1、独立重复事件；2、重复次数...
    </summary>
  

  </entry>

  
  <entry>
    <title>强化学习（动态规划）</title>
    <link href="http://localhost:4000/posts/reinforcement-learning-Dynamic-Programming/" rel="alternate" type="text/html" title="强化学习（动态规划）" />
    <published>2022-11-26T11:24:19+08:00</published>
  
    <updated>2022-11-26T11:24:19+08:00</updated>
  
    <id>http://localhost:4000/posts/reinforcement-learning-Dynamic-Programming/</id>
    <content src="http://localhost:4000/posts/reinforcement-learning-Dynamic-Programming/" />
    <author>
      <name>sirlis</name>
    </author>

  
    
    <category term="Academic" />
    
    <category term="Knowledge" />
    
  

  
    <summary>
      





      本文介绍了强化学习的动态规划法（Dynamic Programming，DP），采用动态规划的思想，分别介绍策略迭代和价值迭代方法。






  1. 强化学习问题的求解
  2. 动态规划
    
      2.1. 策略迭代
        
          2.1.1. 策略评估
          2.1.2. 策略改进
          2.1.3. 策略迭代
        
      
      2.2. 价值迭代
      2.3. 异步动态规划
    
  
  3. 参考文献


1. 强化学习问题的求解

强化学习的最终目标是为了求解最优策略，而最优策略一定对应着最优的价值函数（只要知道了最优价值函数就能很轻松的得到最优策略），因此可将强化学习的目标分解为以下两个基本问题：


  预测问题，也即预测给定策略的状态价值函数。给定强化学习的6...
    </summary>
  

  </entry>

</feed>


